---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
toc: true
toc_depth: 2
---


Summary

Presidential inauguration indicate the beginning of a new four-year term of the president. As one of the traditions, U.S. presidents always address their plans for next four years. This project will offer a broad introduction about word, sentences for presidential inauguration throughout the years, and analyze the differences between Democrat and Republican presidents. 

# Part 1 Data Preparation 

Step 0: check and install needed packages. Load the libraries and functions. 
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels","shiny")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
#sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("shiny")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.
```{r, message=FALSE, warning=FALSE}
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
inaug=f.speechlinks(main.page)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
```

Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap. 
```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
```

Step 3: scrap the texts of speeches from the speech URLs.
```{r}
speech.list=inaug.list
speech.url=inaug
speech.list$type=rep("inaug", nrow(inaug.list))
speech.list=cbind(speech.list, speech.url)
```

```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/fulltext/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```


# Part 2 Overview of sentence length distribution by different types of speeches.  

In this part, we first genrate a sentence length distribution for all presidential inauguration. Then, we compare the sentence length distribution for Republican presidents and democrat presidents. 

```{r, message=FALSE, warning=FALSE}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Some non-sentences exist in raw data due to erroneous extra end-of sentence marks. 
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 
```


Step 1: list out presidents names  

Names list(all Predsidents)
```{r}
sel.comparison=unique(speech.list$President)[-1]
sel.comparison
```

Names list(Republican Predsidents)
```{r}
sel.comparison.republican=unique(speech.list$President[speech.list$Party == "Republican"])[-1]
sel.comparison.republican
```

Names list(Republican Predsidents)
```{r}
sel.comparison.democratic=unique(speech.list$President[speech.list$Party == "Democratic"])[-1]
sel.comparison.democratic
```

Step 2: Name list cleaning and processing 
```{r}
# remove space, speical symbols and eliminate duplicates 
sel.comparison.republican=gsub(" ","",sel.comparison.republican)
sel.comparison.republican=gsub(".","",sel.comparison.republican,fixed = TRUE)
sel.comparison.republican=gsub("*(2)","",sel.comparison.republican,fixed = TRUE)
sel.comparison.republican=unique(gsub("*(3)","",sel.comparison.republican,fixed = TRUE))

sel.comparison.democratic=gsub(" ","",sel.comparison.democratic)
sel.comparison.democratic=gsub(".","",sel.comparison.democratic,fixed = TRUE)
sel.comparison.democratic=gsub("*(2)","",sel.comparison.democratic,fixed = TRUE)
sel.comparison.democratic=unique(gsub("*(3)","",sel.comparison.democratic,fixed = TRUE))

sel.comparison=gsub(" ","",sel.comparison)
sel.comparison=gsub(".","",sel.comparison,fixed = TRUE)
sel.comparison=gsub("*(2)","",sel.comparison,fixed = TRUE)
sel.comparison=unique(gsub("*(3)","",sel.comparison,fixed = TRUE))
```

Step 3: Sentence Length distribution 
```{r, fig.width = 5, fig.height = 5}
sentence.list.sel=sentence.list%>%filter(type=="inaug", File%in%sel.comparison, Term==1)
sentence.list.sel$File=factor(sentence.list.sel$File)

sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, 
                                  sentence.list.sel$word.count, 
                                  mean, 
                                  order=T)
par(mar=c(4, 11, 2, 2))

beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
         cex=0.55, cex.axis=0.4, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         xlim= c(0,150),
         main="Inaugural Speeches--all")
```
The figure above shows that the sentence lentgh of speeches for each president vary. 
Guess: The main reason may be caused my different speech styples for every president, different education background (i.e. law, arts vs science degree)  or coming from two parties.

Let's see the sentence length for Repubican presidents: 

```{r, fig.width = 5, fig.height = 5}
sentence.list.sel.republican=sentence.list%>%filter(type=="inaug", File%in%sel.comparison.republican, Term==1)
sentence.list.sel.republican$File=factor(sentence.list.sel.republican$File)

sentence.list.sel.republican$FileOrdered=reorder(sentence.list.sel.republican$File, 
                                  sentence.list.sel.republican$word.count, 
                                  mean, 
                                  order=T)
par(mar=c(4, 11, 2, 2))

beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel.republican,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
         cex=0.55, cex.axis=0.4, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel.republican$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         xlim= c(0,150),
         main="Inaugural Speeches--Republican")
```


Here is the sentence length for Democratic presidents: 
```{r, fig.width = 5, fig.height = 5}
sentence.list.sel.democratic=sentence.list%>%filter(type=="inaug", File%in%sel.comparison.democratic, Term==1)
sentence.list.sel.democratic$File=factor(sentence.list.sel.democratic$File)

sentence.list.sel.democratic$FileOrdered=reorder(sentence.list.sel.democratic$File, 
                                  sentence.list.sel.democratic$word.count, 
                                  mean, 
                                  order=T)
par(mar=c(4, 11, 2, 2))

beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel.democratic,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
         cex=0.55, cex.axis=0.4, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel.democratic$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         xlim= c(0,150),
         main="Inaugural Speeches--Democratic")
```


Most of the sentence length is within 0-50. For both parties, only one president has sentence length > 100. This indicate the sentence length difference is not caused by party difference. For the education background, thinking as President Obama (with JD degree ) vs Predsident Trump (with Bachelor degree in Economics), the figures do not show huge differences between their speeches. The conclusion is that the sentence speech difference might indicate personal speech style for every president. 

# Part 3 sentiment analysis

In this part, I will do the sentiment analysis to see whether it has emotion difference betweeen two parties. 

Step 1  Clustering of emotions
Democrat
```{r, fig.width=5, fig.height=5}
heatmap.2(cor(sentence.list.sel.democratic%>%filter(type=="inaug")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

par(mar=c(4, 6, 2, 1))
emo.means.democratic=colMeans(select(sentence.list.sel.democratic, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means.democratic[order(emo.means.democratic)], las=2, col=col.use[order(emo.means.democratic)], horiz=T, main="Inaugural Speeches--Democratic")
```

Republican
```{r, fig.width=5, fig.height=5}
heatmap.2(cor(sentence.list.sel.republican%>%filter(type=="inaug")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

par(mar=c(4, 6, 2, 1))
emo.means.republican=colMeans(select(sentence.list.sel.republican, anger:trust)>0.01)
col.use.republican=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means.republican[order(emo.means.republican)], las=2, col=col.use[order(emo.means.republican)], horiz=T, main="Inaugural Speeches--Republican")
```

```{r}
emo.means.democratic
```
```{r}
emo.means.republican
```

Based on barcharts, we could see that both parties focus on trust and joy a lot. However, Presidents from Repulican party talked less with fear and anger emotion. One of the possible guess is due to facing economics recession or not. Only one Democrat President, President Obama, faces 2008 Finanical crisis. However, a lot of Republicans presidents faced economic recession while they gave the speeches. 

Let's dig into some fact about economic recession in U.S. 
Here are the list for recession year in U.S. 
December 1969 - Novemeber 1970 
November 1973 -March 1975 
July 1981 - Nov 1982
July 1990 - March 1991 
March 2001 - November 2001 
December 2007 - June 2009 

All recessions above were preceded by an invertied term structure. Which president faced those economic recession? 
Republian: Richard Milhous Nixon, 1969-1974
           Gerald Rudolph Ford, 1974-1977
           Ronald Wilson Reagan, 1981-1989
           George Herbert Walker Bush, 1989-1993
           George Walker Bush, 2001-2009
Democrat:  Barack Hussein Obama, 2009-2017  


Next step, we will use clustering to see whether president facing economic recession have similar emotions during the speech. 

Democrat
```{r, fig.height=5, fig.width=5}
presid.summary.democratic=tbl_df(sentence.list.sel.democratic)%>%
  filter(type=="inaug", File%in%sel.comparison.democratic)%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
  )

presid.summary.democratic=as.data.frame(presid.summary.democratic)
rownames(presid.summary.democratic)=as.character((presid.summary.democratic[,1]))
km.res.democratic=kmeans(presid.summary.democratic[,-1], iter.max=200,
              5)
fviz_cluster(km.res.democratic, 
             stand=F, repel= TRUE,
             data = presid.summary.democratic[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

Republican
```{r, fig.height=5, fig.width=5}
presid.summary.republican=tbl_df(sentence.list.sel.republican)%>%
  filter(type=="inaug", File%in%sel.comparison.republican)%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    
  )

presid.summary.republican=as.data.frame(presid.summary.republican)
rownames(presid.summary.republican)=as.character((presid.summary.republican[,1]))
km.res.republican=kmeans(presid.summary.republican[,-1], iter.max=200,
              5)
fviz_cluster(km.res.republican, 
             stand=F, repel= TRUE,
             data = presid.summary.republican[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

For the clustering above, we could see majority of Republican presidents who facing economics crisis during their term or at the begining of their terms clustered into same group, which clearly indicates that U.S economcy condition (i.e. GDP) have great effect on presidential inauguration. Since Republican have more presidents facing economic crisis, we dig into the sentiment analysis detail for Republican presidents: 

```{r}
presid.summary.republican
```
Based on the result above, we could clearly see that presidents who facing economic crisis focused more on joy, which encouraged citizens. On the other side, they prefer to use less words and sentences to express anger and fear to aviod citizens' potential disaggrement and dissatisfaction due to terrible economic situations. 

# Part 4 Word Cloud Analysis 

With sentiment analysis, we want to explore more about what presidents talked about in their speeches.

Step 0 - Install and load libraries
```{r, message=FALSE, warning=FALSE}
packages.used=c("tm", "wordcloud", "RColorBrewer", 
                "dplyr", "tydytext")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))

# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
```

Step 1 - Read in the speeches
```{r}
# Republican
folder.path.republican="../data/inauguralsRep/"
speeches.republican=list.files(path = folder.path.republican, pattern = "*.txt")
prex.out.republican=substr(speeches.republican, 6, nchar(speeches.republican)-4)
ff.all.republican<-Corpus(DirSource(folder.path.republican))
```

```{r}
# Democrat
folder.path.democratic="../data/inauguralsDem/"
speeches.democratic=list.files(path = folder.path.democratic, pattern = "*.txt")
prex.out.democratic=substr(speeches.democratic, 6, nchar(speeches.democratic)-4)
ff.all.democratic<-Corpus(DirSource(folder.path.democratic))
```

Step 2 - Text processing

```{r}
# Republican
ff.all.republican<-tm_map(ff.all.republican, stripWhitespace)
ff.all.republican<-tm_map(ff.all.republican, content_transformer(tolower))
ff.all.republican<-tm_map(ff.all.republican, removeWords, stopwords("english"))
ff.all.republican<-tm_map(ff.all.republican, removeWords, character(0))
ff.all.republican<-tm_map(ff.all.republican, removePunctuation)
tdm.all.republican<-TermDocumentMatrix(ff.all.republican)

tdm.tidy.republican=tidy(tdm.all.republican)

tdm.overall.republican=summarise(group_by(tdm.tidy.republican, term), sum(count))
```

```{r}
# Democrat
ff.all.democratic<-tm_map(ff.all.democratic, stripWhitespace)
ff.all.democratic<-tm_map(ff.all.democratic, content_transformer(tolower))
ff.all.democratic<-tm_map(ff.all.democratic, removeWords, stopwords("english"))
ff.all.democratic<-tm_map(ff.all.democratic, removeWords, character(0))
ff.all.democratic<-tm_map(ff.all.democratic, removePunctuation)
tdm.all.democratic<-TermDocumentMatrix(ff.all.democratic)

tdm.tidy.democratic=tidy(tdm.all.democratic)

tdm.overall.democratic=summarise(group_by(tdm.tidy.democratic, term), sum(count))
```

Step 3 - Inspect an overall wordcloud
```{r, fig.height=3, fig.width=3}
# Republican
wordcloud(tdm.overall.republican$term, tdm.overall.republican$`sum(count)`,
          scale=c(3,0.5),
          max.words=300,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```

```{r, fig.height=3, fig.width=3}
# Democrat
wordcloud(tdm.overall.democratic$term, tdm.overall.democratic$`sum(count)`,
          scale=c(3,0.5),
          max.words=300,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```

The above two word clouds shows the word frequency in the speeches. First comes from Republican presidents' speeches and the second comes from Democrat presidents' speeches. We could see both parties mentioned goverment and constitution a lot,however, Republican also talked more  about business and life than Demoncrat. On the other side, Demoncrat expressed more about foreign affairs and war. Keywords "business" and "life" occurs more in Republican presidents' speeches becuase more Republican presidents facing recession during the terms. They had to emphasize on how to boom economy and generate new fortune for people and whole nation. For Demoncrat presidents, 3 Demoncrat Presidents, President Wilson, President Roosevelt and President Truman,  had led U.S. go through World War I and World War II, which leads war topic account a relatively heavy weight in their speeches.

# Part 4 Word Cloud Analysis w/ Rshiny for Individual Speech 

Step 1 - Read in the speeches
```{r}
folder.path="../data/inaugurals/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prex.out=substr(speeches, 6, nchar(speeches)-4)

ff.all<-Corpus(DirSource(folder.path))
```

Step 2 - Text processing

```{r}
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
tdm.all<-TermDocumentMatrix(ff.all)

tdm.tidy=tidy(tdm.all)

tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
```

Step 3 - compute TF-IDF weighted document-term matrices for individual speeches. 
As we would like to identify interesting words for each inaugural speech, we use [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to weigh each term within each speech. It highlights terms that are more specific for a particular speech. 

```{r}
dtm <- DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm=tidy(dtm)
```

Step 4- Interactive visualize important words in individual speeches
```{r, warning=FALSE}
shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1',
                              speeches,
                              selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', speeches,
                              selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3,
                               min = 20, max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),

    server = function(input, output, session) {

      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(input$speech1)],
             dtm.count1=ff.dtm$count[ff.dtm$document==as.character(input$speech1)],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(input$speech2)],
             dtm.count2=ff.dtm$count[ff.dtm$document==as.character(input$speech2)])
      })

      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech2)
      })
    },

    options = list(height = 600)
)
```

# Part 5 Topic Modeling 

In this part, we will analysis what topics presidents from two parties talked about. 

For topic modeling, we prepare a corpus of sentence snipets as follows. For each speech, we start with sentences and prepare a snipet with a given sentence with the flanking sentences. 

```{r}
#Republican
corpus.list.republican=sentence.list.sel.republican[2:(nrow(sentence.list.sel.republican)-1), ]
sentence.pre.republican=sentence.list.sel.republican$sentences[1:(nrow(sentence.list.sel.republican)-2)]
sentence.post.republican=sentence.list.sel.republican$sentences[3:(nrow(sentence.list.sel.republican)-1)]
corpus.list.republican$snipets=paste(sentence.pre.republican, corpus.list.republican$sentences, sentence.post.republican, sep=" ")
rm.rows.republican=(1:nrow(corpus.list.republican))[corpus.list.republican$sent.id==1]
rm.rows.republican=c(rm.rows.republican, rm.rows.republican-1)
corpus.list.republican=corpus.list.republican[-rm.rows.republican, ]
```

```{r}
# Democrat
corpus.list.democratic=sentence.list.sel.democratic[2:(nrow(sentence.list.sel.democratic)-1), ]
sentence.pre.democratic=sentence.list.sel.democratic$sentences[1:(nrow(sentence.list.sel.democratic)-2)]
sentence.post.democratic=sentence.list.sel.democratic$sentences[3:(nrow(sentence.list.sel.democratic)-1)]
corpus.list.democratic$snipets=paste(sentence.pre.democratic, corpus.list.democratic$sentences, sentence.post.democratic, sep=" ")
rm.rows.democratic=(1:nrow(corpus.list.democratic))[corpus.list.democratic$sent.id==1]
rm.rows.democratic=c(rm.rows.democratic, rm.rows.democratic-1)
corpus.list.democratic=corpus.list.democratic[-rm.rows.democratic, ]
```

Text mining
```{r}
#Republican
docs.republican <- Corpus(VectorSource(corpus.list.republican$snipets))
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))
```
```{r}
# Democrat
docs.democratic <- Corpus(VectorSource(corpus.list.democratic$snipets))
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))
```

Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r}
#Republican
#remove potentially problematic symbols
docs.republican <-tm_map(docs.republican,content_transformer(tolower))
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))

#remove punctuation
docs.republican <- tm_map(docs.republican, removePunctuation)
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))

#Strip digits
docs.republican <- tm_map(docs.republican, removeNumbers)
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))

#remove stopwords
docs.republican <- tm_map(docs.republican, removeWords, stopwords("english"))
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))

#remove whitespace
docs.republican <- tm_map(docs.republican, stripWhitespace)
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))

#Stem document
docs.republican<- tm_map(docs.republican,stemDocument)
writeLines(as.character(docs.republican[[sample(1:nrow(corpus.list.republican), 1)]]))
```
```{r}
# Democrat
#remove potentially problematic symbols
docs.democratic <-tm_map(docs.democratic,content_transformer(tolower))
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))

#remove punctuation
docs.democratic <- tm_map(docs.democratic, removePunctuation)
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))

#Strip digits
docs.democratic <- tm_map(docs.democratic, removeNumbers)
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))

#remove stopwords
docs.democratic <- tm_map(docs.democratic, removeWords, stopwords("english"))
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))

#remove whitespace
docs.democratic <- tm_map(docs.democratic, stripWhitespace)
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))

#Stem document
docs.democratic<- tm_map(docs.democratic,stemDocument)
writeLines(as.character(docs.democratic[[sample(1:nrow(corpus.list.democratic), 1)]]))
```


Topic modeling

Gengerate document-term matrices. 

```{r}
#Republican
dtm.republican <- DocumentTermMatrix(docs.republican)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm.republican) <- paste(corpus.list.republican$type, corpus.list.republican$File,
                       corpus.list.republican$Term, corpus.list.republican$sent.id, sep="_")

rowTotals.republican <- apply(dtm.republican , 1, sum) #Find the sum of words in each Document

dtm.republican  <- dtm.republican[rowTotals.republican> 0, ]
corpus.list.republican=corpus.list.republican[rowTotals.republican>0, ]

```

```{r}
# Democrat
dtm.democratic <- DocumentTermMatrix(docs.democratic)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm.democratic) <- paste(corpus.list.democratic$type, corpus.list.democratic$File,
                       corpus.list.democratic$Term, corpus.list.democratic$sent.id, sep="_")

rowTotals.democratic <- apply(dtm.democratic , 1, sum) #Find the sum of words in each Document

dtm.democratic  <- dtm.democratic[rowTotals.democratic> 0, ]
corpus.list.democratic=corpus.list.democratic[rowTotals.democratic>0, ]
```

Run LDA

```{r}
#Republican
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut.republican <-LDA(dtm.republican, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#write out results
#docs to topics
ldaOut.topics.republican <- as.matrix(topics(ldaOut.republican))
table(c(1:k, ldaOut.topics.republican))
write.csv(ldaOut.topics.republican,file=paste("../output/LDAGibbs",k,"DocsToTopicsRepublican.csv"))

#top 6 terms in each topic
ldaOut.terms.republican <- as.matrix(terms(ldaOut.republican,20))
write.csv(ldaOut.terms.republican,file=paste("../output/LDAGibbs",k,"TopicsToTermsRepublican.csv"))

#probabilities associated with each topic assignment
topicProbabilities.republican <- as.data.frame(ldaOut.republican@gamma)
write.csv(topicProbabilities.republican,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesRepublican.csv"))
```

```{r}
# Democrat
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut.democratic <-LDA(dtm.democratic, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#write out results
#docs to topics
ldaOut.topics.democratic <- as.matrix(topics(ldaOut.democratic))
table(c(1:k, ldaOut.topics.democratic))
write.csv(ldaOut.topics.democratic,file=paste("../output/LDAGibbs",k,"DocsToTopicsDemocratic.csv"))

#top 6 terms in each topic
ldaOut.terms.democratic <- as.matrix(terms(ldaOut.democratic,20))
write.csv(ldaOut.terms.democratic,file=paste("../output/LDAGibbs",k,"TopicsToTermsDemocratic.csv"))

#probabilities associated with each topic assignment
topicProbabilities.democratic <- as.data.frame(ldaOut.democratic@gamma)
write.csv(topicProbabilities.democratic,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesDemocratic.csv"))
```



```{r}
#Republican
terms.beta.republican=ldaOut.republican@beta
terms.beta.republican=scale(terms.beta.republican)
topics.terms.republican=NULL
for(i in 1:k){
  topics.terms.republican=rbind(topics.terms.republican, ldaOut.republican@terms[order(terms.beta.republican[i,], decreasing = TRUE)[1:7]])
}
topics.terms.republican
ldaOut.terms.republican
```

Based on the result above, we could see that Republican presidents talked about defenses, tax, justice, freedom, misc, unity,education, peace, foregin affairs,trade, government, legislation, election, patriotism, america

```{r}
# Democrat
terms.beta.democratic=ldaOut.democratic@beta
terms.beta.democratic=scale(terms.beta.democratic)
topics.terms.democratic=NULL
for(i in 1:k){
  topics.terms.democratic=rbind(topics.terms.democratic, ldaOut.democratic@terms[order(terms.beta.democratic[i,], decreasing = TRUE)[1:7]])
}
topics.terms.democratic
ldaOut.terms.democratic
```

Based on the result above, we could see that Republican presidents talked about labor, government, belief, liberty ,patriotism,justice, misc, social benefit, defenses & foreign affairs, election, america, democrat, history, unity, legislation.

Topics modeling results for two parties shows that Republican focus on economy and tax more than democrat since four presidents faced economic recession and tax is one of the fiscal tools to adjust economy in the short term (e.g. about cut tax rate to boost economy). Democrat may foucs on belief, government, country history and unity with more war background. 

# Conculsion

Based on the analysis above, we could get presidents from two parties have different emotions and topics emphasis on throughout the years. Republican presidents put more attention on economy and avoid fearness or other negative emotions during economy recession. Since several Democrat presidents countered war (i.e. world war) during their term, Democrat presidents mentioned more about foreign affairs, peace and war in the speeches. Overall, presidents from two parties talked about government, unity and belief, which are the foundation for U.S.


# Appendix: 

supplement code in python: because running LDA and other topic modeling in R is time-consuming, I used nltk and scikit-learn package in python to do some data exploration (i.e. LDA and n-gram). Beside that, I also used package genism for Hierarchical Dirichlet process, an unsupervised topic modelling method to explore how many topics the speeches for each parties cover. Due to the consistency of results in same programming language, I do not put python part in the report but still include the python code under the folder as appendix. 