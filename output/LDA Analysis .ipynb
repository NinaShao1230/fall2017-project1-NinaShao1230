{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import glob\n",
    "import os \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over the list getting each file\n",
    "#Demoncrat \n",
    "path = '/Users/ninashao/Documents/GitHub/fall2017-project1-NinaShao1230/data/inauguralsDem'\n",
    "text_dem = [] \n",
    "texts_dem = []\n",
    "for fle in glob.glob(os.path.join(path, '*.txt')):\n",
    "    f = open(fle,\"r\")\n",
    "    ftxt = f.read()\n",
    "    text_dem.append(ftxt)\n",
    "    \n",
    "texts_dem ='/n'.join(text_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Republican \n",
    "path = '/Users/ninashao/Documents/GitHub/fall2017-project1-NinaShao1230/data/inauguralsRep'\n",
    "text_rep = [] \n",
    "texts_rep = []\n",
    "for fle in glob.glob(os.path.join(path, '*.txt')):\n",
    "    f = open(fle,\"r\")\n",
    "    ftxt = f.read()\n",
    "    text_rep.append(ftxt)\n",
    "    \n",
    "texts_rep ='/n'.join(text_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % int(topic_idx))\n",
    "        print(\" \".join([feature_names[i]+' '+str(round(topic[i],2))+'\\n' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Republican\n",
    "n_samples_rep = len(text_rep)\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_samples_rep = text_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in                                              0.057s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,min_df=2,max_features=n_features,stop_words='english')\n",
    "t0 = time()\n",
    "tf_rep = tf_vectorizer.fit_transform(data_samples_rep)\n",
    "print(\"done in %50.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features,n_samples=22 and n_features=1000...\n",
      "done in 0.119s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "states 15.23\n",
      " constitution 12.76\n",
      " shall 11.05\n",
      " law 10.48\n",
      " congress 9.15\n",
      " union 8.94\n",
      " laws 8.28\n",
      " national 6.54\n",
      " proper 6.22\n",
      " business 6.08\n",
      " public 5.74\n",
      " united 5.72\n",
      " administration 5.46\n",
      " right 5.39\n",
      " citizens 5.3\n",
      " power 5.15\n",
      " make 5.09\n",
      " country 5.08\n",
      " authority 5.01\n",
      " policy 4.84\n",
      "\n",
      "Topic #1:\n",
      "country 1.63\n",
      " america 1.57\n",
      " freedom 1.4\n",
      " united 1.39\n",
      " time 1.38\n",
      " shall 1.38\n",
      " hope 1.37\n",
      " make 1.37\n",
      " law 1.31\n",
      " believe 1.31\n",
      " states 1.3\n",
      " questions 1.29\n",
      " citizens 1.28\n",
      " know 1.27\n",
      " years 1.26\n",
      " today 1.22\n",
      " justice 1.22\n",
      " political 1.21\n",
      " story 1.21\n",
      " force 1.21\n",
      "\n",
      "Topic #2:\n",
      "public 1.34\n",
      " laws 1.22\n",
      " day 1.18\n",
      " shall 1.17\n",
      " war 1.15\n",
      " influence 1.15\n",
      " citizens 1.14\n",
      " constitution 1.14\n",
      " subject 1.13\n",
      " country 1.13\n",
      " law 1.13\n",
      " progress 1.12\n",
      " political 1.09\n",
      " years 1.08\n",
      " free 1.08\n",
      " states 1.08\n",
      " freedom 1.07\n",
      " present 1.07\n",
      " help 1.06\n",
      " men 1.06\n",
      "\n",
      "Topic #3:\n",
      "country 3.87\n",
      " states 3.83\n",
      " public 2.82\n",
      " citizens 2.63\n",
      " laws 2.39\n",
      " law 2.36\n",
      " constitution 2.29\n",
      " party 2.24\n",
      " shall 2.21\n",
      " united 2.11\n",
      " justice 2.08\n",
      " free 2.05\n",
      " best 2.03\n",
      " american 1.92\n",
      " political 1.92\n",
      " freedom 1.87\n",
      " office 1.85\n",
      " respect 1.84\n",
      " years 1.8\n",
      " prosperity 1.79\n",
      "\n",
      "Topic #4:\n",
      "congress 1.12\n",
      " shall 1.12\n",
      " secure 1.08\n",
      " country 1.07\n",
      " arise 1.05\n",
      " use 1.04\n",
      " order 1.04\n",
      " light 1.04\n",
      " states 1.03\n",
      " execution 1.02\n",
      " break 1.02\n",
      " freedom 1.01\n",
      " years 1.01\n",
      " liberty 1.01\n",
      " power 1.0\n",
      " law 1.0\n",
      " real 1.0\n",
      " century 0.98\n",
      " instrumentalities 0.97\n",
      " powers 0.97\n",
      "\n",
      "Topic #5:\n",
      "america 28.23\n",
      " freedom 27.4\n",
      " country 27.03\n",
      " shall 20.67\n",
      " time 19.02\n",
      " free 17.63\n",
      " let 16.64\n",
      " make 16.5\n",
      " war 15.77\n",
      " citizens 15.75\n",
      " states 15.27\n",
      " american 15.21\n",
      " god 15.12\n",
      " years 14.92\n",
      " public 14.8\n",
      " law 14.55\n",
      " justice 13.71\n",
      " hope 13.68\n",
      " work 13.04\n",
      " life 12.94\n",
      "\n",
      "Topic #6:\n",
      "business 1.06\n",
      " gold 1.06\n",
      " reality 1.0\n",
      " constitution 1.0\n",
      " proud 0.99\n",
      " learn 0.98\n",
      " having 0.98\n",
      " amendments 0.98\n",
      " responsible 0.97\n",
      " shall 0.97\n",
      " declaration 0.96\n",
      " mind 0.96\n",
      " vast 0.95\n",
      " accepted 0.95\n",
      " cherish 0.95\n",
      " promoted 0.95\n",
      " proper 0.95\n",
      " like 0.94\n",
      " increase 0.94\n",
      " blessings 0.93\n",
      "\n",
      "Topic #7:\n",
      "states 1.94\n",
      " shall 1.86\n",
      " law 1.73\n",
      " constitution 1.65\n",
      " laws 1.57\n",
      " free 1.55\n",
      " freedom 1.45\n",
      " citizens 1.43\n",
      " constitutional 1.41\n",
      " country 1.41\n",
      " union 1.39\n",
      " america 1.37\n",
      " men 1.37\n",
      " power 1.37\n",
      " hope 1.36\n",
      " war 1.31\n",
      " justice 1.3\n",
      " public 1.29\n",
      " seek 1.28\n",
      " purpose 1.28\n",
      "\n",
      "Topic #8:\n",
      "war 2.17\n",
      " god 1.45\n",
      " shall 1.39\n",
      " union 1.39\n",
      " laws 1.36\n",
      " states 1.35\n",
      " country 1.33\n",
      " let 1.33\n",
      " years 1.27\n",
      " law 1.27\n",
      " constitution 1.26\n",
      " freedom 1.25\n",
      " right 1.25\n",
      " just 1.24\n",
      " public 1.2\n",
      " come 1.14\n",
      " american 1.11\n",
      " united 1.11\n",
      " progress 1.11\n",
      " office 1.11\n",
      "\n",
      "Topic #9:\n",
      "life 1.48\n",
      " power 1.43\n",
      " men 1.39\n",
      " justice 1.33\n",
      " know 1.32\n",
      " country 1.29\n",
      " problems 1.29\n",
      " responsibility 1.26\n",
      " duty 1.26\n",
      " spirit 1.25\n",
      " america 1.22\n",
      " free 1.21\n",
      " american 1.2\n",
      " purpose 1.18\n",
      " tasks 1.16\n",
      " cause 1.16\n",
      " republic 1.15\n",
      " common 1.14\n",
      " industrial 1.14\n",
      " congress 1.14\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features,\"\n",
    "     \"n_samples=%d and n_features=%d...\"\n",
    "     % (n_samples_rep,n_features))\n",
    "lda_rep = LatentDirichletAllocation(n_topics=n_topics,max_iter=5,learning_method='online',\n",
    "                                learning_offset=50.,random_state=0)\n",
    "t0 = time()\n",
    "lda_rep.fit(tf_rep)\n",
    "print(\"done in %0.3fs.\"%(time()-t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names_rep = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_rep,tf_feature_names_rep,n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist_unnormalized_rep = np.matrix(lda_rep.transform(tf_rep))\n",
    "doc_topic_dist_rep = doc_topic_dist_unnormalized_rep/doc_topic_dist_unnormalized_rep.sum(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [0]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [0]\n",
      " [5]\n",
      " [5]\n",
      " [0]\n",
      " [5]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "doc_topic_rep = doc_topic_dist_rep.argmax(axis=1)\n",
    "print(doc_topic_dist_rep.argmax(axis=1))\n",
    "# this indicate most Republican president focuas on topic 0 and topic 5 in their speeches \n",
    "# topic 0 and topic 5 have higher probability to mention in each speech "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demoncrat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples_dem = len(text_dem)\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "data_samples_dem = text_dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in                                              0.155s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,min_df=2,max_features=n_features,stop_words='english')\n",
    "t0 = time()\n",
    "tf_dem = tf_vectorizer.fit_transform(data_samples_dem)\n",
    "print(\"done in %50.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features,n_samples=22 and n_features=1000...\n",
      "done in 0.145s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "speedily 15.23\n",
      " conferred 12.76\n",
      " serving 11.05\n",
      " labors 10.48\n",
      " concerns 9.15\n",
      " understand 8.94\n",
      " land 8.28\n",
      " midst 6.54\n",
      " proclaim 6.22\n",
      " brought 6.08\n",
      " proud 5.74\n",
      " understood 5.72\n",
      " adherence 5.46\n",
      " right 5.39\n",
      " choose 5.3\n",
      " policy 5.15\n",
      " living 5.09\n",
      " continent 5.08\n",
      " avoided 5.01\n",
      " place 4.84\n",
      "\n",
      "Topic #1:\n",
      "continent 1.63\n",
      " america 1.57\n",
      " genius 1.4\n",
      " understood 1.39\n",
      " territories 1.38\n",
      " serving 1.38\n",
      " improvement 1.37\n",
      " living 1.37\n",
      " labors 1.31\n",
      " beloved 1.31\n",
      " speedily 1.3\n",
      " purpose 1.29\n",
      " choose 1.28\n",
      " justly 1.27\n",
      " yes 1.26\n",
      " terror 1.22\n",
      " justification 1.22\n",
      " places 1.21\n",
      " spread 1.21\n",
      " friend 1.21\n",
      "\n",
      "Topic #2:\n",
      "proud 1.34\n",
      " land 1.22\n",
      " covenant 1.18\n",
      " serving 1.17\n",
      " want 1.15\n",
      " intelligence 1.15\n",
      " choose 1.14\n",
      " conferred 1.14\n",
      " state 1.13\n",
      " continent 1.13\n",
      " labors 1.13\n",
      " principle 1.12\n",
      " places 1.09\n",
      " yes 1.08\n",
      " generous 1.08\n",
      " speedily 1.08\n",
      " genius 1.07\n",
      " possessions 1.07\n",
      " hostile 1.06\n",
      " man 1.06\n",
      "\n",
      "Topic #3:\n",
      "continent 3.87\n",
      " speedily 3.83\n",
      " proud 2.82\n",
      " choose 2.63\n",
      " land 2.39\n",
      " labors 2.36\n",
      " conferred 2.29\n",
      " opportunities 2.24\n",
      " serving 2.21\n",
      " understood 2.11\n",
      " justification 2.08\n",
      " generous 2.05\n",
      " bind 2.03\n",
      " american 1.92\n",
      " places 1.92\n",
      " genius 1.87\n",
      " noble 1.85\n",
      " resolve 1.84\n",
      " yes 1.8\n",
      " progress 1.79\n",
      "\n",
      "Topic #4:\n",
      "concerns 1.12\n",
      " serving 1.12\n",
      " schools 1.08\n",
      " continent 1.07\n",
      " appropriate 1.05\n",
      " unprecedented 1.04\n",
      " offer 1.04\n",
      " legitimate 1.04\n",
      " speedily 1.03\n",
      " expression 1.02\n",
      " borne 1.02\n",
      " genius 1.01\n",
      " yes 1.01\n",
      " legal 1.01\n",
      " policy 1.0\n",
      " labors 1.0\n",
      " read 1.0\n",
      " centuries 0.98\n",
      " international 0.97\n",
      " political 0.97\n",
      "\n",
      "Topic #5:\n",
      "america 28.23\n",
      " genius 27.4\n",
      " continent 27.03\n",
      " serving 20.67\n",
      " territories 19.02\n",
      " generous 17.63\n",
      " left 16.64\n",
      " living 16.5\n",
      " want 15.77\n",
      " choose 15.75\n",
      " speedily 15.27\n",
      " american 15.21\n",
      " guide 15.12\n",
      " yes 14.92\n",
      " proud 14.8\n",
      " labors 14.55\n",
      " justification 13.71\n",
      " improvement 13.68\n",
      " working 13.04\n",
      " legislative 12.94\n",
      "\n",
      "Topic #6:\n",
      "brought 1.06\n",
      " guided 1.06\n",
      " ready 1.0\n",
      " conferred 1.0\n",
      " property 0.99\n",
      " law 0.98\n",
      " hold 0.98\n",
      " ambitions 0.98\n",
      " respects 0.97\n",
      " serving 0.97\n",
      " cultivate 0.96\n",
      " mean 0.96\n",
      " value 0.95\n",
      " abundance 0.95\n",
      " changed 0.95\n",
      " problem 0.95\n",
      " proclaim 0.95\n",
      " lessons 0.94\n",
      " initiative 0.94\n",
      " blood 0.93\n",
      "\n",
      "Topic #7:\n",
      "speedily 1.94\n",
      " serving 1.86\n",
      " labors 1.73\n",
      " conferred 1.65\n",
      " land 1.57\n",
      " generous 1.55\n",
      " genius 1.45\n",
      " choose 1.43\n",
      " confidence 1.41\n",
      " continent 1.41\n",
      " understand 1.39\n",
      " america 1.37\n",
      " man 1.37\n",
      " policy 1.37\n",
      " improvement 1.36\n",
      " want 1.31\n",
      " justification 1.3\n",
      " proud 1.29\n",
      " sections 1.28\n",
      " proved 1.28\n",
      "\n",
      "Topic #8:\n",
      "want 2.17\n",
      " guide 1.45\n",
      " serving 1.39\n",
      " understand 1.39\n",
      " land 1.36\n",
      " speedily 1.35\n",
      " continent 1.33\n",
      " left 1.33\n",
      " yes 1.27\n",
      " labors 1.27\n",
      " conferred 1.26\n",
      " genius 1.25\n",
      " right 1.25\n",
      " justice 1.24\n",
      " proud 1.2\n",
      " claim 1.14\n",
      " american 1.11\n",
      " understood 1.11\n",
      " principle 1.11\n",
      " noble 1.11\n",
      "\n",
      "Topic #9:\n",
      "legislative 1.48\n",
      " policy 1.43\n",
      " man 1.39\n",
      " justification 1.33\n",
      " justly 1.32\n",
      " continent 1.29\n",
      " present 1.29\n",
      " respective 1.26\n",
      " drawn 1.26\n",
      " source 1.25\n",
      " america 1.22\n",
      " generous 1.21\n",
      " american 1.2\n",
      " proved 1.18\n",
      " support 1.16\n",
      " capital 1.16\n",
      " republican 1.15\n",
      " comfort 1.14\n",
      " instrument 1.14\n",
      " concerns 1.14\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features,\"\n",
    "     \"n_samples=%d and n_features=%d...\"\n",
    "     % (n_samples_dem,n_features))\n",
    "lda_dem = LatentDirichletAllocation(n_topics=n_topics,max_iter=5,learning_method='online',\n",
    "                                learning_offset=50.,random_state=0)\n",
    "t0 = time()\n",
    "lda_dem.fit(tf_rep)\n",
    "print(\"done in %0.3fs.\"%(time()-t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names_dem = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_dem,tf_feature_names_dem,n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_dist_unnormalized_dem = np.matrix(lda_dem.transform(tf_dem))\n",
    "doc_topic_dist_dem = doc_topic_dist_unnormalized_dem/doc_topic_dist_unnormalized_dem.sum(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "doc_topic_dem = doc_topic_dist_dem.argmax(axis=1)\n",
    "print(doc_topic_dist_dem.argmax(axis=1))\n",
    "# this indicate most demoncratpresidents focuas on topic 0 and topic 5 in their speeches \n",
    "# topic 5 have higher probability to mention in each speech "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
